\documentclass{llncs}
\usepackage{subcaption}
\usepackage{subfig} 
\usepackage{usual}
\usepackage[utf8]{inputenc}

\usepackage{graphicx}
\usepackage[rflt]{floatflt}
\usepackage[noend]{algpseudocode}
\usepackage{subfig} 
\usepackage{frame, caption}
\usepackage{amsmath}
\usepackage{eulervm}
\usepackage{fontenc}
\usepackage{mathrsfs}
\usepackage{multirow, enumitem, longtable, rotating,lipsum, scrextend}
\usepackage{array}
\usepackage{makecell}
\usepackage{xcolor, soul}
\sethlcolor{yellow}	
\usepackage{floatrow}
\newcommand{\argmax}{\operatornamewithlimits{arg\,max}}
%\pagestyle{plain}
%
\begin{document}
	plan of the paper
	1. We defined a model of dialogue make a schema that explains 
	The model produces an utterance based on the agent mental state
	the agent mental state includes : its preferences, its representation of power noted\emph{pow}, and the context of the dialogue(history of the dialogue)
	
	Previous studies validated our hypotheses that the choice of the utterance as built reflect the right behaviors of power.
	Our goal is to produce plausible social behaviors when building a relation of dominance with a human interlocutor. 
	(Reprendre la definition de dom comme ecrite) 
	
	We make the assumption that our model faithfully reproduce behaviors of power when selecting utterances to enunciate. On that assumption, we aim to build a model of ToM based on simulation, able to compute the current value of power \emph{pow} expressed by the interlocutor based on the utterance expressed. 
	
	\section{Model of dialogue}
		
		We defined a dialogue system of cooperative negotiation which enable  a conversational agent to create and adapt its negotiation strategy to the power it intends to express. The goal of a negotiation is to choose an option from the discussed topic. Theses options are defined as a set of criteria  $\{C_1, ..., C_n\}$ that reflect the option's characteristics. In order to be able to negotiate about which option to choose, an agent is initiated with a set of partial or total ordered set preferences $\prec_i$ on each criterion $C_i$. Using theses preferences, an agent can compute a score of satisfaction for each value of each criterion. The satisfaction of a value $v \in C_i$ is computed as the number of values that the agent prefers less in the set of preferences $\prec_i$, then, the score is normalized in $[0, 1]$: 
		
				\begin{equation}
				sat(v, \prec_i) =	1 - \left( \frac{|\{v' : v' \neq v \  \wedge \ (v \prec_i v')\}| }{( |C_i| - 1 )}\right)
				\end{equation}
				
		The notion of satisfaction represents the score of liking for the value. The closer the satisfaction of a value $v$ gets to 1, the more the agent likes $v$. 
		
		 \subsection{Model of decision based on power}
		The decisional process during the negotiation is built to take into account the power of the agent. Therefore, agent is initiated with a value of power $pow \in [0,1]$. In addition, the decisional process to produce an utterance considers respectively, the agent's power, its preferences and the context of the negotiation.  
		
		A detailed explanation of the decisional model is presented in \cite{} . We present in the following the most important factors that defines the agent strategy.
		
		\subsubsection{Satisfiability }
		The agent is allowed to share its preferences. %(\emph{StatePreference(v)}). 
		To this end, the agent considers its perception of power $pow$, such that a value $v$ is satisfiable (likable), if:  
		\begin{equation}
		 	sat(v, \prec_i) \geq pow
		\end{equation}
	
		We note $S$ the set of satisfiable values given a fixed value of $pow$.
		
		\par For example, for  a mental state where $pow =0.6$, the criterion which domain is  $D =\{A, B, C, D\}$ is defined with the set of preferences $\{A \rightarrow  B, C \rightarrow  D , B \rightarrow D \}$. The values of satisfiability are depicted in the table \ref{sat}. We can compute that the set of satisfiable values is composed by the value $ S = \{B, C, D\}$ 
			 \begin{table}
			 	\centering
			 	\begin{tabular}{ |c|c|c|c|c| }
			 		\hline				
			 		value & A & B & C & D \\
			 		\hline

			 		\hline
			 		Sat(value) & 0.3 & 0.6 & 0.6 & 1 \\
			 		\hline
			 		
			 	\end{tabular}
			 	\caption{Value of satisfiability for the model $D$.}
			 	\label{sat}
			 \end{table}
		
		
	\subsubsection{Acceptability}
	 
	 During the negotiation, the agent makes decisions about the proposals it receives. It might express an \emph{Accept} or \emph{Reject}. However, when the negotiation is not converging, the agent has to make concessions which means that the agent has to lower his level of demand. 
	 We compute the notion of concession with $self(t)$, which is a time varying value of $pow$ that decreases over time $t$. In the beginning, $self(0) = pow$, when the negotiation evolves without converging self decreases $self(t) < pow$ as presented in the figure ... 
	 
	Thus, a proposal with a value $v \in C_i$ is \emph{acceptable} ($v \in Ac$) is computed with a boolean function:
			\begin{equation}
				acc(pow, v) = sat(v, \prec_i) \geq self(t)
			\end{equation}	
	and we note $Ac$ the set of acceptable values.

	At a moment $t \not = 0$, where $self(t) < pow$,  an agent might accept proposals which are not satisfiables.  We note $M$ the set of acceptable values which are not satisfiables.
	$\{v \in M / v \in Ac, v \notin S\}$.
	
%	Therefore, in order to accept \emph{Accept} or propose \emph{Propose} a value $v$, this value has to be acceptable $v \in Acc$. In the contrary, a value $v$ can be rejected (\emph{Reject}) if $v \not \in Acc$ and by consequences $v\not \in S$
%	
\subsubsection{Choice of an utterance}
	The agent uses a strategy to choose the utterance type that reflects behaviors of power. Indeed, high-power agents focus on utterances of \emph{propose} in order to make the negotiation go on. On the contrary, a low-power agent uses in average more \emph{ask} utterances in order to have an accurate knowledge about the other to take the fairest decision.
	
	\subsection{Beliefs about other}
	
	In order to build a complementary relation of dominance with the user, the agent has to construct beliefs about the behaviors of power expressed by the user and adapt to complement his behavior. 
	
	We make the assumption that our model of dialogue effectively presents the process of utterance's selection using power. 
	
	Based on this assumption, we propose to enable the agent with a model of theory of mind based on simulation \cite{bibid}. The agent uses its model of decision in order to guess the behavior of the user from its enunciated utterance.   
	
	The agent needs to infer hypotheses about the user's mental state (\emph{i.e} Pow, Preferences) in order to reason about its behaviors. With knowledge raised during the negotiation, the agent will remove inconsistent hypotheses.
	
	First, we define hypotheses about the possible values of $pow$ that the agents aims to predict. Let $H_{pow} = \{0.1, 0.2, \ldots, 0.9\}$ be the hypotheses on $pow$.
	
	Second, for each fixed hypothesis $ h_i \in H_{pow}$, we define hypotheses on the different set of preferences  noted $M_H $. Thus, for each $C_i$, we compute all the possible combination of preference's relations. When relations of preferences are total ordered, the total number of preference's relations is in the order of $|H{C_i}| = |C_i|!$. In the case of partial ordered relations of preferences, the total number of possible relations is  $ |H{C_i}| = (|C_i| + 1)!$. Hence, for a topic of negotiation with $n$ criteria, the number of possible set of preferences is $ |M_H| = \prod_{i=1}^n (|H{C_i}|)$. 
	
	For each hypothesis $ h_i \in H_{pow}$, we associate the set of possible preferences $M_H(h_i)$ identical for all the hypotheses in the beginning. After each user's turn, the agent updates its hypotheses and remove the ones that did not produced the same utterance than enunciated by the user. The value of power selected is computed as follow:
	
	\begin{equation}
		pow_{Other} = \operatorname*{arg\,max}_{h_i \in H_{pow}} ( M_H(h_i))
	\end{equation} 
	
	This solution presents a computational limitation concerning the number of hypotheses formulated to generate the preferences. As presented, the size of hypotheses $M_H$ is considerable which can slow down the dialogue generation. However, we don't aim to know the "correct" preferences of the user but only the expressed power $pow$. 
	
	To deal with this limitation, we propose to infer hypotheses with partial knowledge about preferences. We only need to represent information about the user's preferences to be able reproduce its decisional model. 
	
	
	\subsubsection{Partial model of preferences}
		The model of preferences is used to compute the satisfiability of values. Knowing the satisfiability us crucial for the decisional process. Therefore, instead of computing all the possible set of preferences, we propose to compute hypotheses about the set of satisfiable values for a giving value of power $pow$.   
	
	
	
	
	

	
	
	
	
\end{document}