\documentclass{llncs}
\usepackage{subcaption}
\usepackage{subfig} 
\usepackage{usual}
\usepackage{graphicx}
\usepackage[rflt]{floatflt}
%\pagestyle{plain}
%
\begin{document}
\title{\vskip -10pt Report on the implementation of the ToM}

\maketitle 

\section{Decisional function}

We aim to extend our model of dialogue to take into account the ToM. The purpose is to guess the other representation of power from the available knowledge gathered from the dialogue (enunciated utterances).
Thus, we have to analyze which factors determine the choice of an utterance. Based on our model of the dialogue, the choice of the next utterance to enunciate is computed with a function called $computeUtt(Model, u^{-1}, pow) $ that takes as input the following information:
\begin{enumerate}
	\item $model$: Agent's mental state in terms of the agent preferences and the current context of the dialogue (proposals, statements ...).
	
	\item $u^{-1}$ the previous utterance enunciated by the other agent.
	
	\item $pow$ the relation of power of the agent. 
	
\end{enumerate}
With this function, we are able to determine which elements affects the choice of the agent's next utterance. Therefore,
in order to guess the power of the other agent from its enunciated utterance, we propose to add to the agent mental state, a set of hypotheses on the  possible mental state of other agent. $H_ms= \{<Model_1, Pow1>, \ldots, <Model_n, pow_n> \}$. After each negotiation turn, when other enunciates an utterance, we call the decisional function on all the elements of $H_ms$ and we only keep the models that produced the same utterance enunciated by the other agent.
This operation is repeated after each dialogue turn until all the remained elements in $H_ms$ are defined with the same values of power $pow_1 = pow_2 = \ldots = pow_n$.

I decomposed the implementation of the ToM in several steps. 

	\subsubsection{Step 1:} We assume that the agent knows the model of preferences of the other agent. thus we make hypotheses only on the other value of power. 
	$H_ms= \{<Model_{other}, Pow1>, <Model_{other}, Pow2>\ldots, <Model_{other}, pow_n> \}$.
	We call after each turn $computeUtt$ on each model of $H_ms$. An example of execution is given in \fig{ex}. Agent 1 is initiated with $pow =0.9$ and Agent 2 is initiated with $pow =0.4$. Agent1 is implemented with the function to guess the power of Agent 2 after each turn of Agent2.
	We can observe that after the first utterance, the algorithm is able to guess that Agent 2 is in the submissive range of power. However, at the end, two values are kept $0.4,0.5$ because they generate the same utterances.
	 \begin{figure} [h]
	 	\centerline{\includegraphics[width=5in]{figs/ex.png}}
	 	\vskip 8pt
	 	\defig{ex}{Example of dialogue between Agent 1 with pow =0.9 and Agent 2 with pow=0.4}
	 \end{figure}
	 
	
	\subsubsection{Step 2:} Once the function was tested and the agent was able to guess the power of other. I wrote the code to generate all the possible model of preferences for a given topic of negotiation. An agent has a set of preferences for each criterion. The combination of all possible preferences is of the order of $n!$. 
	
	In addition, we have to take into account the combination of all the preferences for each criterion to create all the possible preferences models.
	In order to prevent a combinatory explosion due to the large set of all the possible preferences, I ran the previous function several times in order to reproduce the execution of the different model and to calculate the time execution.  

		 \begin{figure} [h]
		 	\centerline{\includegraphics[width=4in]{figs/graph.png}}
		 	\vskip 8pt
		 	\defig{ex}{time execution of the function Guess}
		 \end{figure}

 
 \section{Next step}
 \begin{itemize}
	\item   I have to determine the maximum number of preferences models to have a believable time execution during the dialogue.
	\item Test the agent with the ToM execution.
	\item Profile the code in order to improve the time execution.
 \end{itemize}
 
 I've also made some changes to improve the existing code. 

%\section{Introduction}
%Theory of mind (ToM) is critical for success in social interaction. 
%social agents need ToM to successfully interact with humans.
%Principal challenge with ToM is insincerity  about user preferences and user goal.
%
%\section{Background ToM:}
%Modular theory, 
%simulation theory, 
%theory-theory, 
%and executive function theory.
%
%\section{Model of ToM}

\
\end{document}
